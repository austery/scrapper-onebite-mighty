"""
ä¸»ç¨‹åºå…¥å£
"""
import json
import asyncio
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from playwright.async_api import async_playwright

from config import Config
from login import auto_login, check_login_status
from scraper import load_all_comments, extract_comments
from image_processor import process_images_in_content, process_images_in_content_obsidian, create_markdown_from_html
from obsidian_helpers import (
    parse_relative_time_to_date,
    sanitize_title_for_filename,
    generate_obsidian_filename,
    generate_yaml_frontmatter,
    OBSIDIAN_ARTICLES_DIR,
    OBSIDIAN_ATTACHMENTS_DIR
)
import re
from urllib.parse import urljoin



def extract_post_id(url: str) -> str:
    """
    ä»OneNewBite URLä¸­æå–å¸–å­IDï¼ˆä»…é™äºç®€çŸ­çš„å®‰å…¨æ ‡è¯†ç¬¦ï¼‰
    ä¼˜å…ˆä½¿ç”¨æ•°å­—IDï¼Œé¿å…è¶…é•¿æ–‡ä»¶åé—®é¢˜
    ä¾‹å¦‚: https://onenewbite.com/posts/43168058 -> 43168058
    """
    import hashlib
    from urllib.parse import unquote
    
    # å°è¯•ä»URLä¸­åŒ¹é…æ•°å­—IDï¼ˆæœ€ä¼˜é€‰æ‹©ï¼‰
    match = re.search(r'/posts/(\d+)', url)
    if match:
        return match.group(1)
    
    # å¦‚æœæ²¡æ‰¾åˆ°æ•°å­—IDï¼Œç”ŸæˆåŸºäºURLçš„çŸ­å“ˆå¸ŒID
    # è¿™é¿å…äº†æ–‡ä»¶åè¿‡é•¿çš„é—®é¢˜
    parts = url.rstrip('/').split('/')
    if parts:
        post_part = parts[-1]
        # ä¸ºè¶…é•¿URLç”Ÿæˆå®‰å…¨çš„çŸ­ID
        url_hash = hashlib.md5(post_part.encode('utf-8')).hexdigest()[:12]
        return f"hash_{url_hash}"
    
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºåå¤‡
    return str(int(time.time()))


async def extract_post_id_from_page(page) -> str:
    """
    ä»é¡µé¢HTMLä¸­æå–å¸–å­çš„æ•°å­—ID
    ä¼˜å…ˆä½¿ç”¨URLï¼Œç„¶åæŸ¥æ‰¾DOMå±æ€§
    """
    try:
        # é¦–å…ˆå°è¯•ä»å½“å‰é¡µé¢URLæå–ï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
        current_url = page.url
        print(f"    è°ƒè¯•: å½“å‰é¡µé¢URL: {current_url}")
        id_match = re.search(r'/posts/(\d+)', current_url)
        if id_match:
            extracted_id = id_match.group(1)
            print(f"    è°ƒè¯•: ä»URLæå–åˆ°çš„ID: {extracted_id}")
            return extracted_id
        
        # å¦‚æœURLä¸­æ²¡æ‰¾åˆ°ï¼Œå†å°è¯•DOMé€‰æ‹©å™¨ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        selectors_to_try = [
            '[data-post-id]',
            '[data-id]', 
            '.post[data-id]',
            'article[data-post-id]',
            'article[data-id]',
            '[id*="post"]',
            'meta[property="og:url"]'
        ]
        
        for selector in selectors_to_try:
            try:
                element = await page.query_selector(selector)
                if element:
                    print(f"    è°ƒè¯•: é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å…ƒç´ ")
                    # å°è¯•è·å–data-post-idå±æ€§
                    post_id = await element.get_attribute('data-post-id')
                    if post_id and post_id.isdigit():
                        print(f"    è°ƒè¯•: ä» data-post-id è·å–åˆ°: {post_id}")
                        return post_id
                    
                    # å°è¯•è·å–data-idå±æ€§
                    post_id = await element.get_attribute('data-id')
                    if post_id and post_id.isdigit():
                        print(f"    è°ƒè¯•: ä» data-id è·å–åˆ°: {post_id}")
                        return post_id
                    
                    # å¦‚æœæ˜¯metaæ ‡ç­¾ï¼Œä»contentå±æ€§æå–ID
                    if selector.startswith('meta'):
                        content = await element.get_attribute('content')
                        if content:
                            id_match = re.search(r'/posts/(\d+)', content)
                            if id_match:
                                return id_match.group(1)
            except Exception:
                continue
        
        # å¦‚æœDOMé€‰æ‹©å™¨ä¹Ÿéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›None
            
    except Exception as e:
        print(f"âš ï¸ ä»é¡µé¢æå–IDå¤±è´¥: {e}")
    
    return None


def sanitize_title_for_filename(title: str, max_length: int = 60) -> str:
    """
    æ¸…ç†æ ‡é¢˜å¹¶ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
    """
    if not title:
        return "Untitled"
    
    # ç§»é™¤HTMLæ ‡ç­¾
    import re
    title = re.sub(r'<[^>]+>', '', title)
    
    # æ¸…ç†å¹¶è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    title = re.sub(r'\s+', ' ', title.strip())
    
    # ç§»é™¤æ–‡ä»¶åéæ³•å­—ç¬¦
    illegal_chars = r'[<>:"/\\|?*\x00-\x1f]'
    title = re.sub(illegal_chars, '', title)
    
    # ç§»é™¤å¼€å¤´ç»“å°¾çš„ç‚¹å’Œç©ºæ ¼
    title = title.strip('. ')
    
    # é•¿åº¦æˆªæ–­
    if len(title) > max_length:
        title = title[:max_length].rstrip() + "..."
    
    return title or "Untitled"


def generate_safe_markdown_filename(title: str, published_date: str = None) -> str:
    """
    ç”Ÿæˆå®‰å…¨çš„Markdownæ–‡ä»¶å
    æ ¼å¼: YYYY-MM-DD - [æ¸…ç†å¹¶æˆªæ–­åçš„æ ‡é¢˜].md
    """
    # æ¸…ç†æ ‡é¢˜
    safe_title = sanitize_title_for_filename(title, max_length=60)
    
    # å¦‚æœæ²¡æœ‰æä¾›æ—¥æœŸï¼Œä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸ
    if not published_date:
        from datetime import datetime
        published_date = datetime.now().strftime("%Y-%m-%d")
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = f"{published_date} - {safe_title}.md"
    
    return filename


def get_output_filename(url: str) -> str:
    """
    æ ¹æ®URLç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    """
    post_id = extract_post_id(url)
    return f"post_{post_id}.json"


def is_already_processed(url: str, output_dir: Path) -> bool:
    """
    æ£€æŸ¥URLæ˜¯å¦å·²ç»è¢«å¤„ç†è¿‡ï¼ˆè¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
    """
    filename = get_output_filename(url)
    output_file = output_dir / filename
    return output_file.exists()


def process_post_images_obsidian(post_content: dict, base_url: str) -> dict:
    """å¤„ç†ä¸»å¸–å†…å®¹ä¸­çš„å›¾ç‰‡ï¼ˆObsidianç»Ÿä¸€é™„ä»¶ç®¡ç†æ¨¡å¼ï¼‰"""
    if not post_content or 'content' not in post_content:
        return post_content
    
    processed_content = post_content.copy()
    processed_content['content'] = process_images_in_content_obsidian(
        post_content['content'], base_url
    )
    return processed_content


def process_comment_replies_images_obsidian(replies: list, base_url: str) -> list:
    """é€’å½’å¤„ç†å›å¤ä¸­çš„å›¾ç‰‡ï¼ˆObsidianç»Ÿä¸€é™„ä»¶ç®¡ç†æ¨¡å¼ï¼‰"""
    processed_replies = []
    for reply in replies:
        processed_reply = reply.copy()
        if 'text' in reply:
            processed_reply['text'] = process_images_in_content_obsidian(
                reply['text'], base_url
            )
        # é€’å½’å¤„ç†åµŒå¥—å›å¤
        if 'replies' in reply and reply['replies']:
            processed_reply['replies'] = process_comment_replies_images_obsidian(reply['replies'], base_url)
        processed_replies.append(processed_reply)
    return processed_replies


def process_comments_images_obsidian(comments: list, base_url: str) -> list:
    """å¤„ç†è¯„è®ºä¸­çš„å›¾ç‰‡ï¼ˆObsidianç»Ÿä¸€é™„ä»¶ç®¡ç†æ¨¡å¼ï¼‰"""
    processed_comments = []
    for comment in comments:
        processed_comment = comment.copy()
        if 'text' in comment:
            processed_comment['text'] = process_images_in_content_obsidian(
                comment['text'], base_url
            )
        # å¤„ç†å›å¤
        if 'replies' in comment and comment['replies']:
            processed_comment['replies'] = process_comment_replies_images_obsidian(comment['replies'], base_url)
        processed_comments.append(processed_comment)
    return processed_comments


def process_post_images(post_content: dict, base_url: str, images_folder: Path) -> dict:
    """å¤„ç†ä¸»å¸–å†…å®¹ä¸­çš„å›¾ç‰‡ï¼ˆå‘åå…¼å®¹ï¼‰"""
    if not post_content or 'content' not in post_content:
        return post_content
    
    processed_content = post_content.copy()
    processed_content['content'] = process_images_in_content(
        post_content['content'], base_url, images_folder
    )
    return processed_content


def process_comments_images(comments: list, base_url: str, images_folder: Path) -> list:
    """å¤„ç†è¯„è®ºä¸­çš„å›¾ç‰‡ï¼ˆå‘åå…¼å®¹ï¼‰"""
    processed_comments = []
    for comment in comments:
        processed_comment = comment.copy()
        if 'text' in comment:
            processed_comment['text'] = process_images_in_content(
                comment['text'], base_url, images_folder
            )
        # å¤„ç†å›å¤ï¼ˆå‘åå…¼å®¹ï¼‰
        if 'replies' in comment and comment['replies']:
            processed_comment['replies'] = process_comments_images(comment['replies'], base_url, images_folder)
        processed_comments.append(processed_comment)
    return processed_comments


def render_comment_replies(replies: list, indent_level: int = 1) -> str:
    """é€’å½’æ¸²æŸ“è¯„è®ºå›å¤"""
    if not replies:
        return ""
    
    reply_content = []
    indent = "  " * indent_level  # ç¼©è¿›è¡¨ç¤ºå±‚çº§
    
    for reply in replies:
        if 'author' in reply:
            reply_content.append(f"\n{indent}**{reply['author']}** å›å¤ï¼š\n\n")
        
        if 'text' in reply:
            reply_markdown = create_markdown_from_html(reply['text'])
            # ä¸ºå›å¤å†…å®¹æ·»åŠ ç¼©è¿›
            indented_reply = '\n'.join(f"{indent}{line}" for line in reply_markdown.split('\n'))
            reply_content.append(indented_reply)
        
        if 'timestamp' in reply:
            reply_content.append(f"\n{indent}*å‘å¸ƒæ—¶é—´: {reply['timestamp']}*\n\n")
        
        # é€’å½’å¤„ç†åµŒå¥—å›å¤
        if 'replies' in reply and reply['replies']:
            reply_content.append(render_comment_replies(reply['replies'], indent_level + 1))
    
    return ''.join(reply_content)


def generate_obsidian_markdown_file(post_content: dict, comments: list, url: str, markdown_file: Path):
    """ç”Ÿæˆå®Œå…¨ç¬¦åˆObsidianæ ‡å‡†çš„Markdownæ–‡ä»¶ï¼ŒåŒ…å«YAML frontmatter"""
    markdown_content = []
    
    # ç”ŸæˆYAML frontmatter
    yaml_frontmatter = generate_yaml_frontmatter(post_content, url)
    markdown_content.append(yaml_frontmatter)
    
    # æ·»åŠ æ ‡é¢˜å’Œä¸»å¸–å†…å®¹
    if post_content and 'content' in post_content:
        markdown_content.append("# ä¸»å¸–å†…å®¹\n\n")
        post_markdown = create_markdown_from_html(post_content['content'])
        markdown_content.append(post_markdown)
        markdown_content.append("\n---\n\n")
    
    # æ·»åŠ è¯„è®º
    if comments:
        markdown_content.append("## è¯„è®º\n\n")
        for i, comment in enumerate(comments, 1):
            if 'author' in comment:
                markdown_content.append(f"### {comment['author']}\n\n")
            
            if 'text' in comment:
                comment_markdown = create_markdown_from_html(comment['text'])
                markdown_content.append(comment_markdown)
            
            if 'timestamp' in comment:
                markdown_content.append(f"\n*å‘å¸ƒæ—¶é—´: {comment['timestamp']}*\n\n")
            
            # æ·»åŠ å›å¤
            if 'replies' in comment and comment['replies']:
                markdown_content.append(render_comment_replies(comment['replies']))
            
            markdown_content.append("---\n\n")
    
    # å†™å…¥æ–‡ä»¶
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(''.join(markdown_content))


def generate_markdown_file(post_content: dict, comments: list, markdown_file: Path):
    """å‘åå…¼å®¹çš„Markdownç”Ÿæˆå‡½æ•°"""
    # ä½¿ç”¨æ–°çš„Obsidianå…¼å®¹å‡½æ•°
    url = ""  # æ—§ç‰ˆæœ¬æ²¡æœ‰URLå‚æ•°ï¼Œä½¿ç”¨ç©ºå€¼
    generate_obsidian_markdown_file(post_content, comments, url, markdown_file)


async def check_playwright_installation():
    """
    æ£€æŸ¥ Playwright æµè§ˆå™¨æ˜¯å¦æ­£ç¡®å®‰è£…
    """
    browsers_to_check = [
        ('firefox', 'Firefox'),
        ('chromium', 'Chromium')
    ]
    
    available_browsers = []
    
    for browser_type, browser_name in browsers_to_check:
        try:
            async with async_playwright() as p:
                print(f"ğŸ” æ£€æŸ¥ {browser_name} æµè§ˆå™¨å®‰è£…çŠ¶æ€...")
                if browser_type == 'firefox':
                    browser = await p.firefox.launch(headless=True, timeout=15000)
                else:
                    browser = await p.chromium.launch(
                        headless=True, 
                        args=['--no-sandbox'], 
                        timeout=15000
                    )
                await browser.close()
                print(f"âœ… {browser_name} æµè§ˆå™¨å¯ç”¨")
                available_browsers.append(browser_type)
        except Exception as e:
            print(f"âŒ {browser_name} æµè§ˆå™¨ä¸å¯ç”¨: {e}")
    
    if available_browsers:
        print(f"âœ… å¯ç”¨æµè§ˆå™¨: {', '.join(available_browsers)}")
        return True
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æµè§ˆå™¨")
        print("ğŸ’¡ å°è¯•è¿è¡Œ: playwright install")
        return False


async def process_single_url(url: str) -> dict:
    """
    å¤„ç†å•ä¸ªURLçš„å®Œæ•´æµç¨‹
    """
    print(f"ğŸ” æ­£åœ¨å¤„ç†: {url}")
    
    # å°è¯•ä½¿ç”¨ Firefox è€Œä¸æ˜¯ Chromium
    async with async_playwright() as p:
        # 1. å¯åŠ¨æµè§ˆå™¨
        # macOS ä¼˜åŒ–çš„æµè§ˆå™¨å¯åŠ¨å‚æ•°
        browser_args = [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-web-security', 
            '--disable-extensions',
            '--no-first-run',
            '--disable-default-apps',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--disable-features=TranslateUI',
            '--disable-ipc-flooding-protection',
            '--enable-features=NetworkService,NetworkServiceInProcess',
            '--force-color-profile=srgb',
            '--disable-background-networking',
            '--disable-client-side-phishing-detection',
            '--disable-component-update',
            '--disable-sync',
            '--metrics-recording-only',
            '--no-default-browser-check',
            '--no-service-autorun',
            '--password-store=basic',
            '--use-mock-keychain'
        ]
        
        print("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = None
        
        # å°è¯•ä¸åŒçš„æµè§ˆå™¨å’Œå‚æ•°ç»„åˆ
        browser_attempts = [
            # å°è¯•1: Firefoxï¼ˆé€šå¸¸æ¯” Chromium æ›´ç¨³å®šï¼‰
            {
                'type': 'firefox',
                'args': [],
                'headless': Config.HEADLESS,
                'name': 'Firefox'
            },
            # å°è¯•2: Chromium æœ€å°å‚æ•°
            {
                'type': 'chromium', 
                'args': ['--no-sandbox'],
                'headless': True,
                'name': 'Chromium (æœ€å°å‚æ•°)'
            },
            # å°è¯•3: Chromium å®Œæ•´å‚æ•°
            {
                'type': 'chromium',
                'args': browser_args,
                'headless': Config.HEADLESS,
                'name': 'Chromium (å®Œæ•´å‚æ•°)'
            }
        ]
        
        for i, attempt in enumerate(browser_attempts, 1):
            try:
                print(f"ğŸ”„ å°è¯•å¯åŠ¨ {attempt['name']} (æ–¹æ¡ˆ {i}/{len(browser_attempts)})")
                
                if attempt['type'] == 'firefox':
                    browser = await p.firefox.launch(
                        headless=attempt['headless'],
                        timeout=60000
                    )
                else:  # chromium
                    browser = await p.chromium.launch(
                        headless=attempt['headless'],
                        args=attempt['args'],
                        timeout=60000
                    )
                
                print(f"âœ… {attempt['name']} å¯åŠ¨æˆåŠŸ")
                break
                
            except Exception as e:
                print(f"âŒ {attempt['name']} å¯åŠ¨å¤±è´¥: {e}")
                if i == len(browser_attempts):
                    raise Exception("æ‰€æœ‰æµè§ˆå™¨å¯åŠ¨å°è¯•éƒ½å¤±è´¥äº†")
                continue
        
        try:
            # 2. åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆå°è¯•ä½¿ç”¨å·²ä¿å­˜çš„ä¼šè¯ï¼‰
            context_options = {
                'viewport': {'width': 1920, 'height': 1080},
                'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'ignore_https_errors': True
            }
            
            # å…ˆæ£€æŸ¥ä¼šè¯æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
            if Config.AUTH_FILE.exists():
                print("ğŸ“‚ æ‰¾åˆ°ä¿å­˜çš„ç™»å½•çŠ¶æ€ï¼Œå°è¯•ä½¿ç”¨...")
                try:
                    with open(Config.AUTH_FILE, 'r') as f:
                        storage_state = json.load(f)
                    context_options['storage_state'] = storage_state
                except Exception as e:
                    print(f"âš ï¸  ä¼šè¯æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°ç™»å½•: {e}")
                    if Config.AUTH_FILE.exists():
                        Config.AUTH_FILE.unlink()  # åˆ é™¤æŸåçš„ä¼šè¯æ–‡ä»¶
            
            print("ğŸ”§ åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡...")
            context = await browser.new_context(**context_options)
            print("âœ… æµè§ˆå™¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ")
            
            print("ğŸ“„ åˆ›å»ºæ–°é¡µé¢...")
            # å¢åŠ é‡è¯•æœºåˆ¶
            page = None
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    page = await context.new_page()
                    print("âœ… é¡µé¢åˆ›å»ºæˆåŠŸ")
                    break
                except Exception as e:
                    print(f"âš ï¸ é¡µé¢åˆ›å»ºå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ç­‰å¾…åé‡è¯•...")
                        await asyncio.sleep(2)
                    else:
                        raise Exception(f"é¡µé¢åˆ›å»ºå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
            
            if not page:
                raise Exception("é¡µé¢åˆ›å»ºå¤±è´¥")
            
            # è®¾ç½®è¶…æ—¶
            page.set_default_timeout(Config.TIMEOUT)
            
            # 3. æ£€æŸ¥/æ‰§è¡Œç™»å½•
            login_needed = True
            if Config.AUTH_FILE.exists():
                print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
                login_needed = not await check_login_status(page, Config)
            
            if login_needed:
                print("ğŸ”‘ éœ€è¦é‡æ–°ç™»å½•...")
                login_success = await auto_login(page, Config)
                if not login_success:
                    raise Exception("ç™»å½•å¤±è´¥")
            else:
                print("âœ… å·²ç™»å½•çŠ¶æ€æœ‰æ•ˆ")
            
            # 4. è®¿é—®ç›®æ ‡URL
            print(f"ğŸ“– è®¿é—®ç›®æ ‡é¡µé¢: {url}")
            await page.goto(url, wait_until='networkidle')
            await page.wait_for_timeout(2000)
            
            # 5. Phase 4: å¥å£®çš„IDæå–
            # é¦–å…ˆå°è¯•ä»é¡µé¢HTMLä¸­æå–æ•°å­—ID
            page_post_id = await extract_post_id_from_page(page)
            if page_post_id:
                print(f"âœ… ä»é¡µé¢HTMLæå–åˆ°æ•°å­—ID: {page_post_id}")
                unique_post_id = page_post_id
            else:
                # å¦‚æœé¡µé¢ä¸­æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨URLçš„å®‰å…¨ID
                unique_post_id = extract_post_id(url)
                print(f"ğŸ”§ ä½¿ç”¨URLå®‰å…¨ID: {unique_post_id}")
            
            # 6. æå–ä¸»å¸–å†…å®¹
            from scraper import extract_post_content
            post_content = await extract_post_content(page)
            
            # 7. åŠ è½½æ‰€æœ‰è¯„è®º
            await load_all_comments(page, Config)
            
            # 8. æå–è¯„è®ºæ•°æ®
            comments = await extract_comments(page)
            
            # 9. Phase 4: å®‰å…¨çš„æ–‡ä»¶å‘½åç³»ç»Ÿ
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            OBSIDIAN_ARTICLES_DIR.mkdir(parents=True, exist_ok=True)
            OBSIDIAN_ATTACHMENTS_DIR.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨Obsidianç»Ÿä¸€é™„ä»¶ç®¡ç†æ¨¡å¼å¤„ç†å›¾ç‰‡
            processed_content = process_post_images_obsidian(post_content, url)
            processed_comments = process_comments_images_obsidian(comments, url)
            
            # Phase 4: ä»é¡µé¢å†…å®¹è·å–å¯è¯»æ ‡é¢˜ï¼ˆä¸ä»URLè§£ç ï¼‰
            page_title = processed_content.get('title', '') or 'Untitled Post'
            print(f"ğŸ“ é¡µé¢æ ‡é¢˜: {page_title}")
            
            # Phase 4: ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            relative_time = processed_content.get('timestamp', '')
            published_date = parse_relative_time_to_date(relative_time)
            safe_markdown_filename = generate_safe_markdown_filename(page_title, published_date)
            print(f"ğŸ“„ å®‰å…¨æ–‡ä»¶å: {safe_markdown_filename}")
            
            # Phase 4: ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å
            markdown_file = OBSIDIAN_ARTICLES_DIR / safe_markdown_filename
            
            # ç”Ÿæˆå®Œæ•´çš„Obsidianå…¼å®¹Markdownæ–‡ä»¶
            generate_obsidian_markdown_file(processed_content, processed_comments, url, markdown_file)
            
            # Phase 4: ä½¿ç”¨å”¯ä¸€æ•°å­—IDä½œä¸ºæ–‡ä»¶å¤¹åï¼ˆå‘åå…¼å®¹ï¼‰
            legacy_output_folder = Config.OUTPUT_DIR / unique_post_id
            legacy_output_folder.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºå®Œæ•´çš„è¾“å‡ºæ•°æ®
            output_data = {
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'post': processed_content,
                'total_comments': len(processed_comments),
                'comments': processed_comments
            }
            
            # ä¿å­˜JSONæ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰
            json_file = legacy_output_folder / 'data.json'
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            # æŠ¥å‘Šè¾“å‡ºç»“æœ
            print(f"âœ… Obsidianæ–‡ä»¶å·²ä¿å­˜: {markdown_file.name}")
            print(f"ğŸ“Š æŠ“å–åˆ° {output_data['total_comments']} æ¡è¯„è®º")
            print(f"ğŸ“ è¾“å‡ºä½ç½®:")
            print(f"   ğŸ“„ Obsidianæ–‡ç« : {markdown_file}")
            print(f"   ğŸ“¦ åŸå§‹æ•°æ®: {json_file}")
            
            if OBSIDIAN_ATTACHMENTS_DIR.exists() and any(OBSIDIAN_ATTACHMENTS_DIR.iterdir()):
                image_count = len(list(OBSIDIAN_ATTACHMENTS_DIR.glob('*')))
                print(f"ğŸ–¼ï¸  ç»Ÿä¸€é™„ä»¶åº“: {image_count} ä¸ªå›¾ç‰‡")
            
            return output_data
            
        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        finally:
            try:
                print("ğŸ”„ æ­£åœ¨å…³é—­æµè§ˆå™¨...")
                await browser.close()
                print("âœ… æµè§ˆå™¨å·²å…³é—­")
            except Exception as e:
                print(f"âš ï¸  å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")


async def main():
    """
    ä¸»å‡½æ•°ï¼šä»test_urls.txtè¯»å–URLå¹¶å¤„ç†
    """
    try:
        # æ£€æŸ¥ Playwright å®‰è£…
        if not await check_playwright_installation():
            print("âŒ è¯·å…ˆå®‰è£… Playwright æµè§ˆå™¨")
            print("è¿è¡Œå‘½ä»¤: playwright install chromium")
            return
        
        # éªŒè¯é…ç½®
        print("ğŸ”§ éªŒè¯é…ç½®...")
        Config.validate()
        print("âœ… é…ç½®éªŒè¯é€šè¿‡")
        
        # è¯»å–æµ‹è¯•URL
        # ä¼˜å…ˆä½¿ç”¨test_fresh.txtè¿›è¡Œæ–°æµ‹è¯•
        test_urls_file = Path('test_fresh.txt')
        if not test_urls_file.exists():
            test_urls_file = Path('test_enhanced.txt')
        if not test_urls_file.exists():
            test_urls_file = Path('test_fix.txt')
        if not test_urls_file.exists():
            test_urls_file = Path('test_urls.txt')
            
        if not test_urls_file.exists():
            print("âŒ test_urls.txt æ–‡ä»¶ä¸å­˜åœ¨")
            print("è¯·åˆ›å»º test_urls.txt æ–‡ä»¶å¹¶æ·»åŠ è¦æŠ“å–çš„URL")
            return
        
        with open(test_urls_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
        
        if not urls:
            print("âŒ test_urls.txt ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URL")
            return
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(urls)} ä¸ªURLå¾…å¤„ç†")
        
        # æ£€æŸ¥æ¯ä¸ªURLçš„å¤„ç†çŠ¶æ€
        urls_to_process = []
        skipped_count = 0
        
        for url in urls:
            if is_already_processed(url, Config.OUTPUT_DIR):
                post_id = extract_post_id(url)
                print(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„å¸–å­: {post_id} (æ–‡ä»¶å·²å­˜åœ¨)")
                skipped_count += 1
            else:
                urls_to_process.append(url)
        
        if skipped_count > 0:
            print(f"ï¿½ è·³è¿‡äº† {skipped_count} ä¸ªå·²å¤„ç†çš„URL")
        
        if not urls_to_process:
            print("âœ… æ‰€æœ‰URLéƒ½å·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡æ–°æŠ“å–")
            return
            
        print(f"ğŸ¯ éœ€è¦å¤„ç† {len(urls_to_process)} ä¸ªæ–°URL")
        
        # å¾ªç¯å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„URL
        successful_count = 0
        failed_count = 0
        
        for i, url in enumerate(urls_to_process, 1):
            try:
                post_id = extract_post_id(url)
                print(f"\nğŸš€ å¼€å§‹å¤„ç†ç¬¬ {i}/{len(urls_to_process)} ä¸ªURL...")
                print(f"ğŸ” æ­£åœ¨å¤„ç†å¸–å­: {post_id}")
                print(f"ğŸŒ URL: {url}")
                
                result = await process_single_url(url)
                
                print(f"\nâœ… å¸–å­ {post_id} å¤„ç†å®Œæˆï¼")
                print(f"   è¯„è®ºæ•°: {result['total_comments']}")
                print(f"   ä¿å­˜æ–‡ä»¶: {get_output_filename(url)}")
                
                successful_count += 1
                
                # å¦‚æœè¿˜æœ‰æ›´å¤šURLè¦å¤„ç†ï¼ŒçŸ­æš‚ç­‰å¾…
                if i < len(urls_to_process):
                    print("â³ ç­‰å¾…2ç§’åå¤„ç†ä¸‹ä¸€ä¸ªURL...")
                    await asyncio.sleep(2)
                    
            except Exception as e:
                post_id = extract_post_id(url)
                print(f"\nâŒ å¤„ç†å¸–å­ {post_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                failed_count += 1
                
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªURL
                continue
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   âœ… æˆåŠŸå¤„ç†: {successful_count} ä¸ª")
        print(f"   âŒ å¤„ç†å¤±è´¥: {failed_count} ä¸ª")
        print(f"   â­ï¸ å·²è·³è¿‡: {skipped_count} ä¸ª")
        print(f"   ğŸ“ Obsidianæ–‡ç« ç›®å½•: {OBSIDIAN_ARTICLES_DIR}")
        print(f"   ğŸ–¼ï¸  Obsidiané™„ä»¶ç›®å½•: {OBSIDIAN_ATTACHMENTS_DIR}")
        print(f"   ğŸ“¦ åŸå§‹æ•°æ®ç›®å½•: {Config.OUTPUT_DIR}")
        print(f"\nğŸš€ æ–‡ä»¶å·²å‡†å¤‡å¥½å¯¼å…¥ObsidiançŸ¥è¯†åº“ï¼")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ï¼ˆåœ¨æŸäº›ç³»ç»Ÿä¸Šå¯èƒ½éœ€è¦ï¼‰
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main())