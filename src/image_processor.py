"""
å›¾ç‰‡å¤„ç†æ¨¡å—
å®ç°å›¾ç‰‡å‘ç°ã€ä¸‹è½½å’ŒHTMLè·¯å¾„æ›¿æ¢åŠŸèƒ½
æ”¯æŒObsidianç»Ÿä¸€é™„ä»¶ç®¡ç†
"""
import os
import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from pathlib import Path
from typing import Dict, Any

# Import the unified configuration
from obsidian_helpers import OBSIDIAN_ATTACHMENTS_DIR


def process_images_in_content_obsidian(html_content: str, base_url: str) -> str:
    """
    å¤„ç†HTMLå†…å®¹ä¸­çš„å›¾ç‰‡ï¼ˆObsidianç»Ÿä¸€é™„ä»¶ç®¡ç†æ¨¡å¼ï¼‰
    
    Args:
        html_content: åŸå§‹HTMLå†…å®¹
        base_url: æ–‡ç« çš„åŸºç¡€URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
    
    Returns:
        str: å¤„ç†åçš„HTMLå†…å®¹ï¼Œå›¾ç‰‡è·¯å¾„å·²æ›¿æ¢ä¸ºObsidianå…¼å®¹çš„ç›¸å¯¹è·¯å¾„
    """
    if not html_content or not html_content.strip():
        print("âš ï¸ HTMLå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å›¾ç‰‡å¤„ç†")
        return html_content
    
    print("ğŸ–¼ï¸ å¼€å§‹å¤„ç†HTMLå†…å®¹ä¸­çš„å›¾ç‰‡ï¼ˆObsidianæ¨¡å¼ï¼‰...")
    
    # ç¡®ä¿ç»Ÿä¸€é™„ä»¶æ–‡ä»¶å¤¹å­˜åœ¨
    OBSIDIAN_ATTACHMENTS_DIR.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾
    img_tags = soup.find_all('img')
    
    if not img_tags:
        print("ğŸ“„ æœªæ‰¾åˆ°å›¾ç‰‡æ ‡ç­¾")
        return html_content
    
    print(f"ğŸ” å‘ç° {len(img_tags)} ä¸ªå›¾ç‰‡æ ‡ç­¾")
    
    downloaded_count = 0
    
    for i, img_tag in enumerate(img_tags, 1):
        try:
            # è·å–å›¾ç‰‡URL
            img_url = img_tag.get('src')
            if not img_url:
                print(f"  âš ï¸ ç¬¬ {i} ä¸ªå›¾ç‰‡æ ‡ç­¾æ²¡æœ‰srcå±æ€§ï¼Œè·³è¿‡")
                continue
            
            print(f"  ğŸ“¥ å¤„ç†ç¬¬ {i} ä¸ªå›¾ç‰‡: {img_url}")
            
            # å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
            absolute_img_url = urljoin(base_url, img_url)
            print(f"    ğŸŒ ç»å¯¹URL: {absolute_img_url}")
            
            # ä¸‹è½½å›¾ç‰‡åˆ°ç»Ÿä¸€é™„ä»¶ç›®å½•
            local_filename = download_image_obsidian(absolute_img_url, i)
            
            if local_filename:
                # æ›¿æ¢HTMLä¸­çš„å›¾ç‰‡è·¯å¾„ä¸ºObsidianå…¼å®¹çš„ç›¸å¯¹è·¯å¾„
                # ä» articles/ ç›®å½•æŒ‡å‘ attachments/ ç›®å½•
                img_tag['src'] = f'../attachments/{local_filename}'
                downloaded_count += 1
                print(f"    âœ… å·²ä¸‹è½½å¹¶æ›´æ–°è·¯å¾„: ../attachments/{local_filename}")
            else:
                print(f"    âŒ ä¸‹è½½å¤±è´¥ï¼Œä¿æŒåŸè·¯å¾„")
                
        except Exception as e:
            print(f"    âŒ å¤„ç†ç¬¬ {i} ä¸ªå›¾ç‰‡æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"ğŸ‰ å›¾ç‰‡å¤„ç†å®Œæˆï¼æˆåŠŸä¸‹è½½ {downloaded_count}/{len(img_tags)} ä¸ªå›¾ç‰‡")
    
    # è¿”å›ä¿®æ”¹åçš„HTML
    return str(soup)


def process_images_in_content(html_content: str, base_url: str, images_folder: Path) -> str:
    """
    å¤„ç†HTMLå†…å®¹ä¸­çš„å›¾ç‰‡ï¼šå‘ç°ã€ä¸‹è½½ã€æ›¿æ¢è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
    
    Args:
        html_content: åŸå§‹HTMLå†…å®¹
        base_url: æ–‡ç« çš„åŸºç¡€URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
        images_folder: å›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        str: å¤„ç†åçš„HTMLå†…å®¹ï¼Œå›¾ç‰‡è·¯å¾„å·²æ›¿æ¢ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
    """
    if not html_content or not html_content.strip():
        print("âš ï¸ HTMLå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å›¾ç‰‡å¤„ç†")
        return html_content
    
    print("ğŸ–¼ï¸ å¼€å§‹å¤„ç†HTMLå†…å®¹ä¸­çš„å›¾ç‰‡...")
    
    # ç¡®ä¿å›¾ç‰‡æ–‡ä»¶å¤¹å­˜åœ¨
    images_folder.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾
    img_tags = soup.find_all('img')
    
    if not img_tags:
        print("ğŸ“„ æœªæ‰¾åˆ°å›¾ç‰‡æ ‡ç­¾")
        return html_content
    
    print(f"ğŸ” å‘ç° {len(img_tags)} ä¸ªå›¾ç‰‡æ ‡ç­¾")
    
    downloaded_count = 0
    
    for i, img_tag in enumerate(img_tags, 1):
        try:
            # è·å–å›¾ç‰‡URL
            img_url = img_tag.get('src')
            if not img_url:
                print(f"  âš ï¸ ç¬¬ {i} ä¸ªå›¾ç‰‡æ ‡ç­¾æ²¡æœ‰srcå±æ€§ï¼Œè·³è¿‡")
                continue
            
            print(f"  ğŸ“¥ å¤„ç†ç¬¬ {i} ä¸ªå›¾ç‰‡: {img_url}")
            
            # å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
            absolute_img_url = urljoin(base_url, img_url)
            print(f"    ğŸŒ ç»å¯¹URL: {absolute_img_url}")
            
            # ä¸‹è½½å›¾ç‰‡
            local_filename = download_image(absolute_img_url, images_folder, i)
            
            if local_filename:
                # æ›¿æ¢HTMLä¸­çš„å›¾ç‰‡è·¯å¾„ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
                img_tag['src'] = f'images/{local_filename}'
                downloaded_count += 1
                print(f"    âœ… å·²ä¸‹è½½å¹¶æ›´æ–°è·¯å¾„: images/{local_filename}")
            else:
                print(f"    âŒ ä¸‹è½½å¤±è´¥ï¼Œä¿æŒåŸè·¯å¾„")
                
        except Exception as e:
            print(f"    âŒ å¤„ç†ç¬¬ {i} ä¸ªå›¾ç‰‡æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"ğŸ‰ å›¾ç‰‡å¤„ç†å®Œæˆï¼æˆåŠŸä¸‹è½½ {downloaded_count}/{len(img_tags)} ä¸ªå›¾ç‰‡")
    
    # è¿”å›ä¿®æ”¹åçš„HTML
    return str(soup)


def download_image_obsidian(img_url: str, img_index: int) -> str:
    """
    ä¸‹è½½å•ä¸ªå›¾ç‰‡åˆ°Obsidianç»Ÿä¸€é™„ä»¶ç›®å½•
    
    Args:
        img_url: å›¾ç‰‡URL
        img_index: å›¾ç‰‡ç´¢å¼•ï¼ˆç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼‰
    
    Returns:
        str: æœ¬åœ°æ–‡ä»¶åï¼Œå¦‚æœä¸‹è½½å¤±è´¥è¿”å›None
    """
    try:
        # å‘é€HTTPè¯·æ±‚ä¸‹è½½å›¾ç‰‡
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(img_url, headers=headers, stream=True, timeout=10)
        response.raise_for_status()
        
        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
        parsed_url = urlparse(img_url)
        original_filename = os.path.basename(parsed_url.path)
        
        # å¦‚æœURLä¸­æ²¡æœ‰æ–‡ä»¶åï¼Œç”Ÿæˆä¸€ä¸ª
        if not original_filename or '.' not in original_filename:
            # å°è¯•ä»Content-Typeè·å–æ–‡ä»¶æ‰©å±•å
            content_type = response.headers.get('content-type', '')
            if 'image/jpeg' in content_type or 'image/jpg' in content_type:
                ext = '.jpg'
            elif 'image/png' in content_type:
                ext = '.png'
            elif 'image/gif' in content_type:
                ext = '.gif'
            elif 'image/webp' in content_type:
                ext = '.webp'
            else:
                ext = '.jpg'  # é»˜è®¤æ‰©å±•å
            
            original_filename = f'image_{img_index}{ext}'
        
        # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
        safe_filename = sanitize_filename(original_filename)
        
        # é¿å…æ–‡ä»¶åå†²çª
        local_path = OBSIDIAN_ATTACHMENTS_DIR / safe_filename
        counter = 1
        while local_path.exists():
            name, ext = os.path.splitext(safe_filename)
            safe_filename = f"{name}_{counter}{ext}"
            local_path = OBSIDIAN_ATTACHMENTS_DIR / safe_filename
            counter += 1
        
        # ä¿å­˜å›¾ç‰‡
        with open(local_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"    ğŸ’¾ å›¾ç‰‡å·²ä¿å­˜åˆ°ç»Ÿä¸€é™„ä»¶åº“: {local_path}")
        return safe_filename
        
    except requests.exceptions.RequestException as e:
        print(f"    âŒ ç½‘ç»œé”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"    âŒ ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return None


def download_image(img_url: str, images_folder: Path, img_index: int) -> str:
    """
    ä¸‹è½½å•ä¸ªå›¾ç‰‡ï¼ˆå‘åå…¼å®¹ï¼‰
    
    Args:
        img_url: å›¾ç‰‡URL
        images_folder: å›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹
        img_index: å›¾ç‰‡ç´¢å¼•ï¼ˆç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼‰
    
    Returns:
        str: æœ¬åœ°æ–‡ä»¶åï¼Œå¦‚æœä¸‹è½½å¤±è´¥è¿”å›None
    """
    try:
        # å‘é€HTTPè¯·æ±‚ä¸‹è½½å›¾ç‰‡
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(img_url, headers=headers, stream=True, timeout=10)
        response.raise_for_status()
        
        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
        parsed_url = urlparse(img_url)
        original_filename = os.path.basename(parsed_url.path)
        
        # å¦‚æœURLä¸­æ²¡æœ‰æ–‡ä»¶åï¼Œç”Ÿæˆä¸€ä¸ª
        if not original_filename or '.' not in original_filename:
            # å°è¯•ä»Content-Typeè·å–æ–‡ä»¶æ‰©å±•å
            content_type = response.headers.get('content-type', '')
            if 'image/jpeg' in content_type or 'image/jpg' in content_type:
                ext = '.jpg'
            elif 'image/png' in content_type:
                ext = '.png'
            elif 'image/gif' in content_type:
                ext = '.gif'
            elif 'image/webp' in content_type:
                ext = '.webp'
            else:
                ext = '.jpg'  # é»˜è®¤æ‰©å±•å
            
            original_filename = f'image_{img_index}{ext}'
        
        # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
        safe_filename = sanitize_filename(original_filename)
        
        # é¿å…æ–‡ä»¶åå†²çª
        local_path = images_folder / safe_filename
        counter = 1
        while local_path.exists():
            name, ext = os.path.splitext(safe_filename)
            safe_filename = f"{name}_{counter}{ext}"
            local_path = images_folder / safe_filename
            counter += 1
        
        # ä¿å­˜å›¾ç‰‡
        with open(local_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"    ğŸ’¾ å›¾ç‰‡å·²ä¿å­˜: {local_path}")
        return safe_filename
        
    except requests.exceptions.RequestException as e:
        print(f"    âŒ ç½‘ç»œé”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"    âŒ ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return None


def sanitize_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸å®‰å…¨çš„å­—ç¬¦
    
    Args:
        filename: åŸå§‹æ–‡ä»¶å
    
    Returns:
        str: æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
    """
    # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
    unsafe_chars = '<>:"/\\|?*'
    for char in unsafe_chars:
        filename = filename.replace(char, '_')
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    filename = ''.join(char for char in filename if ord(char) >= 32)
    
    # é™åˆ¶é•¿åº¦
    if len(filename) > 255:
        name, ext = os.path.splitext(filename)
        filename = name[:255-len(ext)] + ext
    
    return filename


def create_markdown_from_html(html_content: str, title: str = "") -> str:
    """
    å°†HTMLå†…å®¹è½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        html_content: HTMLå†…å®¹
        title: æ–‡ç« æ ‡é¢˜
    
    Returns:
        str: Markdownå†…å®¹
    """
    try:
        import re
        from markdownify import markdownify as md
        
        # é¢„å¤„ç†ï¼šç§»é™¤mighty-hashtagé“¾æ¥ï¼Œåªä¿ç•™hashtagæ–‡æœ¬
        # å°† <a class="navigate mighty-hashtag" href="...#hashtag">text</a> è½¬æ¢ä¸ºçº¯æ–‡æœ¬
        processed_html = re.sub(
            r'<a[^>]*class="[^"]*mighty-hashtag[^"]*"[^>]*href="[^"]*"[^>]*>(.*?)</a>',
            r'\1',
            html_content,
            flags=re.IGNORECASE | re.DOTALL
        )
        
        # è½¬æ¢HTMLä¸ºMarkdown
        markdown_content = md(processed_html, heading_style="ATX", bullets="-")
        
        # å¦‚æœæœ‰æ ‡é¢˜ï¼Œæ·»åŠ åˆ°å¼€å¤´
        if title:
            markdown_content = f"# {title}\n\n{markdown_content}"
        
        return markdown_content
        
    except ImportError:
        print("âš ï¸ markdownifyåº“æœªå®‰è£…ï¼Œæ— æ³•è½¬æ¢ä¸ºMarkdown")
        return html_content
    except Exception as e:
        print(f"âŒ è½¬æ¢Markdownæ—¶å‡ºé”™: {e}")
        return html_content