# Comment Scraper Project Specification v3.0

## ğŸ¯ Project Overview

**Project Name:** Automated Comment Scraper  
**Purpose:** è‡ªåŠ¨ç™»å½•ç½‘ç«™ï¼ŒæŠ“å–æŒ‡å®šå¸–å­çš„æ‰€æœ‰è¯„è®ºï¼Œä¿å­˜ä¸ºç»“æ„åŒ–JSONæ–‡ä»¶  
**Language:** Python 3.8+  
**Core Library:** Playwright (async)  
**Mode:** åŠè‡ªåŠ¨åŒ– - è‡ªåŠ¨ç™»å½• + å•å¸–å¤„ç†

## ğŸ“ Project Structure

comment_scraper/
â”œâ”€â”€ claude.md              # æœ¬æ–‡æ¡£
â”œâ”€â”€ .env                   # ç™»å½•å‡­æ®ï¼ˆä¸æäº¤Gitï¼‰
â”œâ”€â”€ .env.example           # å‡­æ®æ¨¡æ¿
â”œâ”€â”€ .gitignore            # Gitå¿½ç•¥é…ç½®
â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–
â”œâ”€â”€ test_urls.txt         # æµ‹è¯•URLåˆ—è¡¨
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ config.py         # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ login.py          # è‡ªåŠ¨ç™»å½•æ¨¡å—
â”‚   â”œâ”€â”€ scraper.py        # è¯„è®ºæŠ“å–æ ¸å¿ƒ
â”‚   â””â”€â”€ main.py           # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ output/               # JSONè¾“å‡ºç›®å½•
â”‚   â””â”€â”€ [timestamp]_[post_id].json
â””â”€â”€ logs/                 # æ—¥å¿—æ–‡ä»¶
    â””â”€â”€ scraper.log


## ğŸ”§ Phase 1: Environment Setup

### 1.1 Dependencies (requirements.txt)

```txt
playwright==1.40.0
python-dotenv==1.0.0
```

### 1.2 Configuration Files

**.env.example** (æ¨¡æ¿ï¼Œå¯æäº¤)

```env
# Website Configuration
SITE_URL=https://example.com
LOGIN_URL=https://example.com/login
USERNAME=your_username_here
PASSWORD=your_password_here

# Scraper Settings
HEADLESS=False
TIMEOUT=30000
WAIT_TIME=500

# Selectors (æ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´)
USERNAME_SELECTOR=input[name="username"]
PASSWORD_SELECTOR=input[name="password"]
LOGIN_BUTTON_SELECTOR=button[type="submit"]
LOGIN_SUCCESS_INDICATOR=.user-avatar
```

**.gitignore** (å¿…é¡»åŒ…å«)

```gitignore
# Sensitive
.env
auth.json
*.session

# Output
output/
logs/

# Python
__pycache__/
*.py[cod]

# IDE
.vscode/
.idea/
```

**test_urls.txt** (æµ‹è¯•ç”¨URL)

```txt
https://example.com/post/12345
```

## ğŸš€ Phase 2: Core Implementation

### 2.1 Configuration Module (src/config.py)

```python
"""
åŠ è½½å’ŒéªŒè¯é…ç½®
"""
import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

class Config:
    # ç½‘ç«™é…ç½®
    SITE_URL = os.getenv('SITE_URL')
    LOGIN_URL = os.getenv('LOGIN_URL')
    USERNAME = os.getenv('USERNAME')
    PASSWORD = os.getenv('PASSWORD')
    
    # é€‰æ‹©å™¨
    USERNAME_SELECTOR = os.getenv('USERNAME_SELECTOR')
    PASSWORD_SELECTOR = os.getenv('PASSWORD_SELECTOR')
    LOGIN_BUTTON_SELECTOR = os.getenv('LOGIN_BUTTON_SELECTOR')
    LOGIN_SUCCESS_INDICATOR = os.getenv('LOGIN_SUCCESS_INDICATOR')
    
    # è®¾ç½®
    HEADLESS = os.getenv('HEADLESS', 'False').lower() == 'true'
    TIMEOUT = int(os.getenv('TIMEOUT', 30000))
    WAIT_TIME = int(os.getenv('WAIT_TIME', 500))
    
    # è·¯å¾„
    OUTPUT_DIR = Path('output')
    LOGS_DIR = Path('logs')
    AUTH_FILE = Path('auth.json')
    
    @classmethod
    def validate(cls):
        """éªŒè¯å¿…è¦é…ç½®"""
        required = ['SITE_URL', 'USERNAME', 'PASSWORD']
        missing = [k for k in required if not getattr(cls, k)]
        if missing:
            raise ValueError(f"ç¼ºå°‘é…ç½®: {', '.join(missing)}")
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        cls.OUTPUT_DIR.mkdir(exist_ok=True)
        cls.LOGS_DIR.mkdir(exist_ok=True)
```

### 2.2 Auto Login Module (src/login.py)

```python
"""
è‡ªåŠ¨ç™»å½•å’Œä¼šè¯ç®¡ç†
"""
async def auto_login(page, config):
    """
    è‡ªåŠ¨ç™»å½•æµç¨‹
    Returns: bool - ç™»å½•æ˜¯å¦æˆåŠŸ
    """
    # 1. è®¿é—®ç™»å½•é¡µ
    # 2. å¡«å†™å‡­æ®
    # 3. ç‚¹å‡»ç™»å½•
    # 4. éªŒè¯ç™»å½•æˆåŠŸ
    # 5. ä¿å­˜ä¼šè¯çŠ¶æ€
    pass

async def check_login_status(page, config):
    """
    æ£€æŸ¥ç™»å½•çŠ¶æ€æ˜¯å¦æœ‰æ•ˆ
    Returns: bool
    """
    pass
```

### 2.3 Scraper Core (src/scraper.py)

#### å…³é”®é€‰æ‹©å™¨ï¼ˆç»å¯¹ä¸èƒ½ä½¿ç”¨nth-childï¼‰

```python
SELECTORS = {
    # é”šç‚¹ï¼šè¯„è®ºåŒºå®¹å™¨
    'COMMENT_CONTAINER': '#sidebar-comments-region',
    
    # åŠ è½½æ›´å¤šè¯„è®ºæŒ‰é’®
    'LOAD_MORE_BUTTON': 'text="Previous Comments"',
    
    # å±•å¼€æŠ˜å å†…å®¹é“¾æ¥
    'EXPAND_LINKS': 'a.more.text-color-grey-3-link',
    
    # è¯„è®ºé¡¹ï¼ˆåœ¨å®¹å™¨å†…æŸ¥æ‰¾ï¼‰
    'COMMENT_ITEMS': 'li',  # å¿…é¡»åœ¨COMMENT_CONTAINERå†…ä½¿ç”¨
}
```

#### åŒå¾ªç¯åŠ è½½ç­–ç•¥

```python
async def load_all_comments(page):
    """
    Phase 1: å¾ªç¯ç‚¹å‡»"Previous Comments"ç›´åˆ°å…¨éƒ¨åŠ è½½
    Phase 2: å¾ªç¯ç‚¹å‡»æ‰€æœ‰"more"é“¾æ¥ç›´åˆ°å…¨éƒ¨å±•å¼€
    """
    # Loop 1: åŠ è½½æ‰€æœ‰è¯„è®ºé¡µ
    while True:
        try:
            button = page.get_by_text('Previous Comments')
            if await button.is_visible():
                await button.click()
                await page.wait_for_load_state('networkidle')
                await page.wait_for_timeout(config.WAIT_TIME)
            else:
                break
        except:
            break
    
    # Loop 2: å±•å¼€æ‰€æœ‰æŠ˜å å†…å®¹
    while True:
        links = await page.locator('a.more.text-color-grey-3-link').all()
        if not links:
            break
        for link in links:
            await link.click()
            await page.wait_for_timeout(config.WAIT_TIME)
```

#### æ•°æ®æå–

```python
async def extract_comments(page):
    """
    æå–è¯„è®ºæ•°æ®ï¼Œä¿æŒå±‚çº§ç»“æ„
    Returns: List[Dict] - è¯„è®ºæ•°æ®ç»“æ„
    """
    # å®šä½è¯„è®ºå®¹å™¨
    container = page.locator('#sidebar-comments-region')
    
    # è·å–æ‰€æœ‰æ ¹è¯„è®º
    comments = []
    root_items = await container.locator('li').all()
    
    for item in root_items:
        comment_data = {
            'text': '',      # è¯„è®ºæ­£æ–‡
            'author': '',    # ä½œè€…ï¼ˆå¦‚æœæœ‰ï¼‰
            'timestamp': '', # æ—¶é—´æˆ³ï¼ˆå¦‚æœæœ‰ï¼‰
            'replies': []    # å›å¤åˆ—è¡¨
        }
        
        # TODO: æå–è¯„è®ºæ–‡æœ¬
        # TODO: æå–å›å¤ï¼ˆåµŒå¥—çš„liæˆ–ç‰¹å®šclassï¼‰
        # TODO: ä¿æŒå±‚çº§å…³ç³»
        
        comments.append(comment_data)
    
    return comments
```

### 2.4 Main Entry (src/main.py)

```python
"""
ä¸»ç¨‹åºå…¥å£
"""
import json
import asyncio
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

async def process_single_url(url):
    """
    å¤„ç†å•ä¸ªURLçš„å®Œæ•´æµç¨‹
    """
    async with async_playwright() as p:
        # 1. å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(headless=Config.HEADLESS)
        
        # 2. åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆå°è¯•ä½¿ç”¨å·²ä¿å­˜çš„ä¼šè¯ï¼‰
        context_options = {'viewport': {'width': 1920, 'height': 1080}}
        if Config.AUTH_FILE.exists():
            context_options['storage_state'] = str(Config.AUTH_FILE)
        
        context = await browser.new_context(**context_options)
        page = await context.new_page()
        
        # 3. æ£€æŸ¥/æ‰§è¡Œç™»å½•
        if not Config.AUTH_FILE.exists() or not await check_login_status(page):
            await auto_login(page, Config)
        
        # 4. è®¿é—®ç›®æ ‡URL
        await page.goto(url)
        
        # 5. åŠ è½½æ‰€æœ‰è¯„è®º
        await load_all_comments(page)
        
        # 6. æå–æ•°æ®
        comments = await extract_comments(page)
        
        # 7. ä¿å­˜ä¸ºJSON
        output_data = {
            'url': url,
            'scraped_at': datetime.now().isoformat(),
            'total_comments': len(comments),
            'comments': comments
        }
        
        # ç”Ÿæˆæ–‡ä»¶åï¼šæ—¶é—´æˆ³_å¸–å­ID.json
        post_id = url.split('/')[-1]
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = Config.OUTPUT_DIR / f"{timestamp}_{post_id}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")
        
        await browser.close()
        return output_data

async def main():
    """
    ä¸»å‡½æ•°ï¼šä»test_urls.txtè¯»å–URLå¹¶å¤„ç†
    """
    # éªŒè¯é…ç½®
    Config.validate()
    
    # è¯»å–æµ‹è¯•URL
    with open('test_urls.txt', 'r') as f:
        urls = [line.strip() for line in f if line.strip()]
    
    if not urls:
        print("âŒ test_urls.txt ä¸­æ²¡æœ‰URL")
        return
    
    # å¤„ç†ç¬¬ä¸€ä¸ªURLä½œä¸ºæµ‹è¯•
    test_url = urls[0]
    print(f"ğŸ” æ­£åœ¨å¤„ç†: {test_url}")
    
    try:
        result = await process_single_url(test_url)
        print(f"ğŸ“Š æŠ“å–åˆ° {result['total_comments']} æ¡è¯„è®º")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“Š Phase 3: Output Format

### 3.1 JSON Structure (å½“å‰é˜¶æ®µ)

```json
{
  "url": "https://example.com/post/12345",
  "scraped_at": "2024-12-20T15:30:00",
  "total_comments": 25,
  "comments": [
    {
      "text": "è¿™æ˜¯ä¸€æ¡æ ¹è¯„è®ºçš„å®Œæ•´å†…å®¹",
      "author": "ç”¨æˆ·A",
      "timestamp": "2024-12-20 14:00",
      "replies": [
        {
          "text": "è¿™æ˜¯å›å¤1",
          "author": "ç”¨æˆ·B",
          "timestamp": "2024-12-20 14:30"
        },
        {
          "text": "è¿™æ˜¯å›å¤2",
          "author": "ç”¨æˆ·C",
          "timestamp": "2024-12-20 14:45"
        }
      ]
    },
    {
      "text": "è¿™æ˜¯å¦ä¸€æ¡æ ¹è¯„è®º",
      "author": "ç”¨æˆ·D",
      "timestamp": "2024-12-20 15:00",
      "replies": []
    }
  ]
}
```

### 3.2 Future: Markdown Format (åç»­å®ç°)

```markdown
# å¸–å­æ ‡é¢˜
URL: https://example.com/post/12345
æŠ“å–æ—¶é—´: 2024-12-20 15:30:00

## è¯„è®º (25)

**ç”¨æˆ·A** - 2024-12-20 14:00
è¿™æ˜¯ä¸€æ¡æ ¹è¯„è®ºçš„å®Œæ•´å†…å®¹

---

> **ç”¨æˆ·B** - 2024-12-20 14:30
> è¿™æ˜¯å›å¤1

> **ç”¨æˆ·C** - 2024-12-20 14:45
> è¿™æ˜¯å›å¤2

=====================================

**ç”¨æˆ·D** - 2024-12-20 15:00
è¿™æ˜¯å¦ä¸€æ¡æ ¹è¯„è®º

=====================================
```

## âš ï¸ Critical Rules

1. **NEVER use nth-child or position-based selectors**
2. **ALWAYS use try-except for element operations**
3. **NEVER commit .env file to Git**
4. **ALWAYS validate login status before scraping**
5. **Log every major operation for debugging**

## ğŸ§ª Testing Strategy

### Step 1: Setup Test

```bash
# 1. å¤åˆ¶é…ç½®
cp .env.example .env
# 2. å¡«å†™çœŸå®å‡­æ®
nano .env
# 3. æ·»åŠ æµ‹è¯•URL
echo "https://example.com/post/12345" > test_urls.txt
```

### Step 2: Run Test

```bash
python src/main.py
```

### Expected Output

```
ğŸ” æ­£åœ¨å¤„ç†: https://example.com/post/12345
æ­£åœ¨ç™»å½•...
âœ… ç™»å½•æˆåŠŸ
åŠ è½½è¯„è®ºä¸­...
å±•å¼€æŠ˜å å†…å®¹...
æå–è¯„è®ºæ•°æ®...
âœ… å·²ä¿å­˜åˆ°: output/20241220_153000_12345.json
ğŸ“Š æŠ“å–åˆ° 25 æ¡è¯„è®º
```

## ğŸ“ Implementation Checklist

- [ ] Create project structure
- [ ] Setup .env configuration
- [ ] Implement auto-login
- [ ] Implement dual-loop loading
- [ ] Implement comment extraction
- [ ] Save to JSON
- [ ] Add error handling
- [ ] Add logging
- [ ] Test with real URL
- [ ] (Future) Add Markdown output

## ğŸš¦ Next Steps

1. **å½“å‰é˜¶æ®µ**: å®ç°JSONè¾“å‡ºçš„åŸºç¡€åŠŸèƒ½
2. **ä¸‹ä¸€é˜¶æ®µ**: ä¼˜åŒ–æ•°æ®æå–ï¼Œç¡®ä¿å±‚çº§ç»“æ„æ­£ç¡®
3. **æœªæ¥é˜¶æ®µ**: æ·»åŠ Markdownè¾“å‡ºæ ¼å¼

